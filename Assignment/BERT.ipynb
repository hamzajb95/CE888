{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Assignment Preprocessing 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamzajb95/CE888/blob/main/Assignment/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91r5NVy12Xi6",
        "outputId": "aeee08fe-67ca-428f-9b6e-cf2bb5792a18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers\r\n",
        "\r\n",
        "#BERT requirements:\r\n",
        "!pip install tqdm boto3 requests regex sentencepiece sacremoses\r\n",
        "\r\n",
        "#!pip install pytorch-lightening==1.1.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 10.3MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=5ad0fb69dcdbf4dfb829d1e2c2bf22c0a18662c426296f97152e9400676fb29d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocSUrWS4QeKq",
        "outputId": "adfb8779-f18e-47d8-cd3c-cd0e5a366ff6"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers as ppb\n",
        "\n",
        "from pathlib import Path\n",
        "import urllib\n",
        "import re\n",
        "import string\n",
        "import math\n",
        "from nltk.corpus import stopwords as stopwords\n",
        "from nltk.stem.porter import *\n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-OoqODHQeKv"
      },
      "source": [
        "#You pass the url to the function and you are returned a list\n",
        "def getText(url):\n",
        "    response = urllib.request.urlopen(url)\n",
        "    Text = response.read()\n",
        "    Text=Text.decode(\"utf-8\")\n",
        "    lines = Text.split('\\n')\n",
        "    return lines[:-1]\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "#Importing data for emotion dataset\n",
        "url_mapping = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/mapping.txt'\n",
        "url_trainText = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/train_text.txt'\n",
        "url_trainLabel = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/train_labels.txt'\n",
        "url_testText = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/test_text.txt'\n",
        "url_testLabel = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/test_labels.txt'\n",
        "\n",
        "emotion_list = [url_mapping,url_trainText,url_trainLabel,url_testText,url_testLabel]\n",
        "\n",
        "def preProcess(urlList):\n",
        "  mapping = getText(urlList[0])\n",
        "  train_text = getText(urlList[1])\n",
        "  train_labels = getText(urlList[2])\n",
        "  test_text = getText(urlList[3])\n",
        "  test_labels = getText(urlList[4])\n",
        "\n",
        "  #Turn those train and test lists into Dataframes.\n",
        "  df=pd.DataFrame({\"tweet\":train_text,\"label\":train_labels}) #change name to df_train\n",
        "  df_test = pd.DataFrame({\"tweet\":test_text, \"label\":test_labels})\n",
        "\n",
        "  #We are now going to remove the @user tags from tweets for both train and test sets.\n",
        "  df['tweet']=df['tweet'].apply(lambda x: re.sub('@user','', x))\n",
        "  df_test['tweet']=df_test['tweet'].apply(lambda x: re.sub('@user','', x))\n",
        "  #We are going to change all words to lower case\n",
        "  df['tweet']=df['tweet'].apply(lambda x: x.lower())\n",
        "  df_test['tweet']=df_test['tweet'].apply(lambda x: x.lower())\n",
        "  #Remove special character from train and test set\n",
        "  df['tweet'] = df['tweet'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "  df_test['tweet'] = df_test['tweet'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "  #Remove all stopwords from the Tweet columns of the datasets\n",
        "  stop = stopwords.words('english')\n",
        "  df['tweet'] = df['tweet'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
        "  df_test['tweet'] = df_test['tweet'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
        "  #Remove any words that have a length less than 4\n",
        "  df['tweet'] = df['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>4]))\n",
        "  df_test['tweet'] = df_test['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>4]))\n",
        "  #We now tokenize the dataframed in an added column\n",
        "  df['tokenized_tweets'] = df.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
        "  df_test['tokenized_tweets'] = df_test.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
        "\n",
        "\n",
        "  return df, df_test"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae5NU7IAQeKw",
        "outputId": "7814016e-5395-412a-c31c-64cf2a834a13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "preProcessed = preProcess(emotion_list)\n",
        "emot_train = preProcessed[0]\n",
        "emot_test = preProcessed[1]\n",
        "\n",
        "print(emot_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                                         tweet  ...                                                                    tokenized_tweets\n",
            "0          worry payment problem never joyce meyer motivation leadership worry  ...       [worry, payment, problem, never, joyce, meyer, motivation, leadership, worry]\n",
            "1                          roommate spell autocorrect terrible firstworldprobs  ...                           [roommate, spell, autocorrect, terrible, firstworldprobs]\n",
            "2                                                probably photos cherry helped  ...                                                  [probably, photos, cherry, helped]\n",
            "3     rooneys fucking untouchable fucking dreadful depay looked decent tonight  ...  [rooneys, fucking, untouchable, fucking, dreadful, depay, looked, decent, tonight]\n",
            "4                                      pretty depressing favourite highlighter  ...                                        [pretty, depressing, favourite, highlighter]\n",
            "...                                                                        ...  ...                                                                                 ...\n",
            "3252            discouraged fucking years contact thousands tweets would tweet  ...             [discouraged, fucking, years, contact, thousands, tweets, would, tweet]\n",
            "3253                                    contention hosting nation camden empty  ...                                        [contention, hosting, nation, camden, empty]\n",
            "3254                                       fellow shiver shallowness arguments  ...                                            [fellow, shiver, shallowness, arguments]\n",
            "3255                                                         problem something  ...                                                                [problem, something]\n",
            "3256                         fight insult lions seriously kinda wasted players  ...                           [fight, insult, lions, seriously, kinda, wasted, players]\n",
            "\n",
            "[3257 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TwDfbNthnoM"
      },
      "source": [
        "#Calling the BERT model, the BERT Tokenizer and choosing the model with the hyperparameters.\r\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KN8v-3DcrWj",
        "outputId": "dbbbea2a-3f32-433d-b5bd-9382e4d5089b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\r\n",
        "tokenized = emot_train['tweet'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\r\n",
        "tokenized"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                       [101, 4737, 7909, 3291, 2196, 11830, 11527, 14354, 4105, 4737, 102]\n",
              "1                          [101, 18328, 6297, 8285, 27108, 2890, 6593, 6659, 2034, 11108, 21572, 5910, 102]\n",
              "2                                                                        [101, 2763, 7760, 9115, 3271, 102]\n",
              "3       [101, 24246, 2015, 8239, 19662, 10875, 3085, 8239, 21794, 2139, 4502, 2100, 2246, 11519, 3892, 102]\n",
              "4                                                          [101, 3492, 2139, 24128, 8837, 12944, 2121, 102]\n",
              "                                                       ...                                                 \n",
              "3252                  [101, 22585, 8239, 2086, 3967, 5190, 1056, 28394, 3215, 2052, 1056, 28394, 2102, 102]\n",
              "3253                                                             [101, 18974, 9936, 3842, 13267, 4064, 102]\n",
              "3254                                                              [101, 3507, 13277, 8467, 2791, 9918, 102]\n",
              "3255                                                                                 [101, 3291, 2242, 102]\n",
              "3256                                                [101, 2954, 15301, 7212, 5667, 17704, 13842, 2867, 102]\n",
              "Name: tweet, Length: 3257, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUCo24TgQeKy",
        "outputId": "c6dee5f9-bdd0-4076-daa9-582cdfb15175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### We need to each list in tokenized to have the same length\n",
        "### We do this by first getting the tweet with the biggest length\n",
        "### Then we \n",
        "maxLen = 0\n",
        "for i in tokenized.values:\n",
        "  if len(i) > maxLen:\n",
        "    maxLen = len(i)\n",
        "\n",
        "paded = np.array([i + [0]*(maxLen-len(i)) for i in tokenized.values])\n",
        "print(paded.shape) #So just to be clear the maxLen was 35 as that is not the length of each numpy array.\n",
        "\n",
        "#We need to create another numpy array which gives 0 to ignore the mask and 1 to accept the Token\n",
        "attention_mask = np.where(paded != 0,1,0)\n",
        "print(attention_matrix.shape)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3257, 35)\n",
            "(3257, 35)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fIhIubhToQh"
      },
      "source": [
        "##Lets run the BERT model by providing the model with the padded and attention mask\r\n",
        "input_ids = torch.tensor(padded)\r\n",
        "attention_mask = torch.tensor(attention_mask)\r\n",
        "#Context Manager that disables gradient calculations (torch.no_grad())\r\n",
        "#model() function runs our sentences through BERT\r\n",
        "with torch.no_grad():\r\n",
        "  last_states = model(input_ids, attention_mask = attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoCep_WVuB3v"
      },
      "source": [
        "Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called `[CLS]` (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\" />\n",
        "\n",
        "We'll save those in the `features` variable, as they'll serve as the features to our logitics regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0ipd7MIXdZt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FosWYeXyQeKz"
      },
      "source": [
        "#Function to create Word Cloud\n",
        "# def createWC(df):\n",
        "#     wordbank = [' '.join(map(str,x)) for x in df['tokenized_tweets']]\n",
        "#     wordbank = \"\".join(wordbank)\n",
        "#     wordbank.strip()\n",
        "    \n",
        "#     wordCloud = WordCloud(width=800,height=500, random_state=1, background_color='salmon').generate(wordbank)\n",
        "#     plt.figure(figsize=(10, 7))\n",
        "#     plt.imshow(wordCloud)\n",
        "#     plt.axis('off')\n",
        "#     plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}