{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNsnKCnKO2ifh0w8Ubp5O6C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b71eee35694d45e382f413fc7e0860c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d2bd87e624d54aaabd79638cba927554",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7c727193445949e78182cfad71c3bf91",
              "IPY_MODEL_ac9ab7b2766e49f7861ff36936d773e3"
            ]
          }
        },
        "d2bd87e624d54aaabd79638cba927554": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7c727193445949e78182cfad71c3bf91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4dc4dd92c6a04809871d522956e09254",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2720f19d03db4a52a89afd130d8aa02a"
          }
        },
        "ac9ab7b2766e49f7861ff36936d773e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7a77b7a7a9374036aa45d0dabc438186",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:02&lt;00:00, 87.9kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb3565aa1d894dfabee41acf5662f385"
          }
        },
        "4dc4dd92c6a04809871d522956e09254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2720f19d03db4a52a89afd130d8aa02a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a77b7a7a9374036aa45d0dabc438186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb3565aa1d894dfabee41acf5662f385": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e5c6a536dda4a75ad84f5a86f83f5f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6c3fbf97bca24c2eb7c3484e38341414",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_94c9c2068c724c9b92d9ff57eacdcac8",
              "IPY_MODEL_d954668c045849528cf46d9612f5429d"
            ]
          }
        },
        "6c3fbf97bca24c2eb7c3484e38341414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "94c9c2068c724c9b92d9ff57eacdcac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cf634502264a4a75ba0109d108dc2833",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 29,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 29,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d9de694ec4134e459f2f57e6c540ff22"
          }
        },
        "d954668c045849528cf46d9612f5429d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ecfa5e158e5f4af3acffc84c17f37acb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29.0/29.0 [00:01&lt;00:00, 28.5B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7f6f7116a0d444f5aaca8f468427ec62"
          }
        },
        "cf634502264a4a75ba0109d108dc2833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d9de694ec4134e459f2f57e6c540ff22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ecfa5e158e5f4af3acffc84c17f37acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7f6f7116a0d444f5aaca8f468427ec62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4eea15228dfb4507b26fcf33879116e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b25a3835f4b846ee9d79d25febe002cd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9224072aba08434ab60f26adae9dc9b5",
              "IPY_MODEL_6f58aacd9bf44548b96525c4eaaece8c"
            ]
          }
        },
        "b25a3835f4b846ee9d79d25febe002cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9224072aba08434ab60f26adae9dc9b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_11a16c1e9b1e4ab3b7b02b59265f99f3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435797,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435797,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c875daef99f34796a9ae777e3f20fa40"
          }
        },
        "6f58aacd9bf44548b96525c4eaaece8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8640c0d26a074311bf1a251bf41bfb2c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436k/436k [00:00&lt;00:00, 975kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_173cfb3578804d33b204dd4243c2ce78"
          }
        },
        "11a16c1e9b1e4ab3b7b02b59265f99f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c875daef99f34796a9ae777e3f20fa40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8640c0d26a074311bf1a251bf41bfb2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "173cfb3578804d33b204dd4243c2ce78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "11ed57caa879459b985d47aff71446d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c23f9bc5770e48d8944ef4b3cf4a3afb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f4b1c639c8e84dd7ae986737a94ab08e",
              "IPY_MODEL_3ab2c3ca6cd343f79a6a5d337e530e87"
            ]
          }
        },
        "c23f9bc5770e48d8944ef4b3cf4a3afb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f4b1c639c8e84dd7ae986737a94ab08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9ef785cf76954103b95420716b4a1e95",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e655717e7fbd4949a6eeae96cf912ff2"
          }
        },
        "3ab2c3ca6cd343f79a6a5d337e530e87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_748cc0a00d1d46fa987fc2a0d76e15a6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:00&lt;00:00, 2.40kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d84ae0d177244b3b87b44d3f5fc66cfa"
          }
        },
        "9ef785cf76954103b95420716b4a1e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e655717e7fbd4949a6eeae96cf912ff2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "748cc0a00d1d46fa987fc2a0d76e15a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d84ae0d177244b3b87b44d3f5fc66cfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1b439de4c7a1486094db4fc9e4fc1cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b139b8e979d94d6497bcf87aae88e15f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_41b86e4b6c84458abe93ca90fa783052",
              "IPY_MODEL_ff06024d402a46d3a1ed537450318f4f"
            ]
          }
        },
        "b139b8e979d94d6497bcf87aae88e15f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41b86e4b6c84458abe93ca90fa783052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_588b25c8d7a24236a62ead8f1d0aa50d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f04d333ffcae4c48bfb57e613072e0de"
          }
        },
        "ff06024d402a46d3a1ed537450318f4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d2eeb29a4118449a86c0107435f51cad",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:12&lt;00:00, 33.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_98ab349d3e924eff99c94e043fa39719"
          }
        },
        "588b25c8d7a24236a62ead8f1d0aa50d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f04d333ffcae4c48bfb57e613072e0de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2eeb29a4118449a86c0107435f51cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "98ab349d3e924eff99c94e043fa39719": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamzajb95/CE888/blob/main/Project/EmotionClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBP6RRww2YYH",
        "outputId": "ca787683-231b-413d-8105-6da23e94e588"
      },
      "source": [
        " !pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1MB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV6XliLW3rcK",
        "outputId": "6185245e-a672-4e62-9b1d-f396f6e95aa2"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Apr 27 14:31:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTB6QExT2jUl"
      },
      "source": [
        "import urllib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import transformers as ppb\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import collections\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import pprint\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09dmB___4mU9"
      },
      "source": [
        "device = torch.device(type='cuda', index=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ktp765S3ASL"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "#Importing data for emotion dataset\n",
        "url_mapping = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/mapping.txt'\n",
        "url_trainText = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/train_text.txt'\n",
        "url_trainLabel = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/train_labels.txt'\n",
        "url_testText = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/test_text.txt'\n",
        "url_testLabel = 'https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/test_labels.txt'\n",
        "\n",
        "emotion_list = [url_mapping,url_trainText,url_trainLabel,url_testText,url_testLabel]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVfa5Idc23Nu"
      },
      "source": [
        "#You pass the url to the function and you are returned a list\n",
        "def getText(url):\n",
        "    response = urllib.request.urlopen(url)\n",
        "    Text = response.read()\n",
        "    Text=Text.decode(\"utf-8\")\n",
        "    lines = Text.split('\\n')\n",
        "    return lines[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev2PNWSY3Kj0"
      },
      "source": [
        "#Removes @users tags from the tweets and return the train and test data\n",
        "def preProcess(urlList):\n",
        "  mapping = getText(urlList[0])\n",
        "  train_text = getText(urlList[1])[:-1]\n",
        "  train_labels = getText(urlList[2])[:-1]\n",
        "  test_text = getText(urlList[3])[:-1]\n",
        "  test_labels = getText(urlList[4])[:-1]\n",
        "\n",
        "  #Turn those train and test lists into Dataframes.\n",
        "  df=pd.DataFrame({\"tweet\":train_text,\"label\":train_labels}) #change name to df_train\n",
        "  df_test = pd.DataFrame({\"tweet\":test_text, \"label\":test_labels})\n",
        "\n",
        "  \n",
        "\n",
        "  #We are now going to remove the @user tags from tweets for both train and test sets.\n",
        "  df['tweet']=df['tweet'].apply(lambda x: re.sub('@user','', x))\n",
        "  df_test['tweet']=df_test['tweet'].apply(lambda x: re.sub('@user','', x))\n",
        "\n",
        "  print(mapping)\n",
        "  \n",
        "  return df, df_test, mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "Ltmpz3gw4KUC",
        "outputId": "019b4cb9-ccd0-4169-b3ba-46fdca181085"
      },
      "source": [
        "emot_data = preProcess(emotion_list)\n",
        "emot_train = emot_data[0]\n",
        "emot_test = emot_data[1]\n",
        "emot_train\n",
        "label_names = ['anger','joy','optimism','sadness']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['0\\tanger', '1\\tjoy', '2\\toptimism', '3\\tsadness']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>â€œWorry is a down payment on a problem you may never have'. Â Joyce Meyer.  #motivation #leadership #worry</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>My roommate: it's okay that we can't spell because we have autocorrect. #terrible #firstworldprobs</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>No but that's so cute. Atsu was probably shy about photos before but cherry helped her out uwu</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Rooneys fucking untouchable isn't he? Been fucking dreadful again, depay has looked decent(ish)tonight</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>it's pretty depressing when u hit pan on ur favourite highlighter</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3252</th>\n",
              "      <td>I get discouraged because I try for 5 fucking years a contact with Lady Gaga but are thousands of tweets, how she would see my tweet? :(</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3253</th>\n",
              "      <td>The  are in contention and hosting  nation and Camden is empty #sad</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3254</th>\n",
              "      <td>as a fellow UP grad, i shiver at the shallowness of his arguments</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3255</th>\n",
              "      <td>You have a #problem? Yes! Can you do #something about it? No! Than why</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3256</th>\n",
              "      <td>i will fight this guy! Don't insult the lions like that! But seriously they kinda are.Wasted some of the best players</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3257 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                          tweet label\n",
              "0                                     â€œWorry is a down payment on a problem you may never have'. Â Joyce Meyer.  #motivation #leadership #worry      2\n",
              "1                                           My roommate: it's okay that we can't spell because we have autocorrect. #terrible #firstworldprobs      0\n",
              "2                                               No but that's so cute. Atsu was probably shy about photos before but cherry helped her out uwu      1\n",
              "3                                       Rooneys fucking untouchable isn't he? Been fucking dreadful again, depay has looked decent(ish)tonight      0\n",
              "4                                                                            it's pretty depressing when u hit pan on ur favourite highlighter      3\n",
              "...                                                                                                                                         ...   ...\n",
              "3252  I get discouraged because I try for 5 fucking years a contact with Lady Gaga but are thousands of tweets, how she would see my tweet? :(      3\n",
              "3253                                                                       The  are in contention and hosting  nation and Camden is empty #sad      3\n",
              "3254                                                                         as a fellow UP grad, i shiver at the shallowness of his arguments      0\n",
              "3255                                                                   You have a #problem? Yes! Can you do #something about it? No! Than why       0\n",
              "3256                     i will fight this guy! Don't insult the lions like that! But seriously they kinda are.Wasted some of the best players      0\n",
              "\n",
              "[3257 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6jfdO1dBOqn"
      },
      "source": [
        "label_names = ['anger','joy','optimism','sadness']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "-aqwTDsR4MlF",
        "outputId": "4138afca-ec16-4f4e-b372-d6449b5fa447"
      },
      "source": [
        "sns.countplot(emot_train.label)\n",
        "plt.xlabel('Emotion');\n",
        "plt.title(\"Label Count Train Set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Label Count Train Set')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "1Oa4w33K7we8",
        "outputId": "41113286-22dd-4126-9fb8-1da99be1e9b7"
      },
      "source": [
        "sns.countplot(emot_test.label)\n",
        "plt.xlabel('Emotion');\n",
        "plt.title(\"Label Count Test Set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Label Count Test Set')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "dHghM7DWE4hF",
        "outputId": "0f443eba-ae2a-4b6b-a2a0-cb8e85784ae9"
      },
      "source": [
        "##  Lets create an Validation set as well from the training set\n",
        "emot_train, emot_val = train_test_split(emot_train, test_size=0.2, random_state = 23, stratify=emot_train['label'])\n",
        "emot_val\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2634</th>\n",
              "      <td>Tired of people pretending Islam isn't one of the most misogynistic religions, it's no coincidence Muslim countries are terrible for women.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1918</th>\n",
              "      <td>We lost</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1233</th>\n",
              "      <td>data stolen in 2014 and only now do you tell us #shocking.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>to give me my keys back. They aren't for my house! #shocking</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2102</th>\n",
              "      <td>which was worse than expected and hilarious too. no one will even remember.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1568</th>\n",
              "      <td>Might be the pout of a star baker tho !</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1795</th>\n",
              "      <td>they irritate me. Them and their inch thick made up masks</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1418</th>\n",
              "      <td>Paul forever. Paul should have won! Paul played such a better game! #BB18</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2077</th>\n",
              "      <td>Will give #sleepnumber another chance for customer service. Today's growl is 15+ minutes on hold. Overall bed + service = very unimpressive.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2860</th>\n",
              "      <td>SPARKLES, proud little huff. Still posing mind you.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>652 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                              tweet label\n",
              "2634   Tired of people pretending Islam isn't one of the most misogynistic religions, it's no coincidence Muslim countries are terrible for women.      0\n",
              "1918                                                                                                                                       We lost      3\n",
              "1233                                                                                    data stolen in 2014 and only now do you tell us #shocking.      0\n",
              "378                                                                                   to give me my keys back. They aren't for my house! #shocking      0\n",
              "2102                                                                   which was worse than expected and hilarious too. no one will even remember.      1\n",
              "...                                                                                                                                             ...   ...\n",
              "1568                                                                                                       Might be the pout of a star baker tho !      1\n",
              "1795                                                                                     they irritate me. Them and their inch thick made up masks      0\n",
              "1418                                                                    Paul forever. Paul should have won! Paul played such a better game! #BB18       1\n",
              "2077  Will give #sleepnumber another chance for customer service. Today's growl is 15+ minutes on hold. Overall bed + service = very unimpressive.      0\n",
              "2860                                                                                           SPARKLES, proud little huff. Still posing mind you.      2\n",
              "\n",
              "[652 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "aarMrjjdG-wr",
        "outputId": "bc91dee1-21a4-416d-83dc-6c77af53062f"
      },
      "source": [
        "sns.countplot(emot_val.label)\n",
        "plt.xlabel('Emotion');\n",
        "plt.title(\"Label Count Validation Set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Label Count Validation Set')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWL0lEQVR4nO3de7hddX3n8fdHQEEBAckgJME4Fm1TL+BEtGKVilOBaqFeEK1AERs7xQJT2hn0eaZQKtWO12otnVgRUIti8YJKtZThMnZEDIhcdYoIJRFIAJEgFkn4zh9rnR+74STZCWeffU7yfj3Pes5av3X7Zu2T/dnrt9ZeJ1WFJEkAjxt3AZKkmcNQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKGgkklyS5K3Tve5slWRBkkqydT/9D0mOGmbZTdjXO5P87WOpV5svQ0HrleSWJK8Ydx2DkjwzyeeS3JXkJ0muSfKHSbYa8X7PTPKu9cz/XpK3TNJ+fJKlG7Ovqjqoqs7alDrX2vf+SZatte0/r6opD90kj0/y/iTLktzf/+58aMh1T0nyqamuSRvPUNCskuQZwLeA24DnVNWTgdcDi4AdxlkbcBZw5CTtR/TzNnfvoHsd9qV7LfYHrhpnQdoEVeXgsM4BuAV4xSTtOwNfAVYCP+7H5w3MvwR4N3AFcB/wJWCXgfkvAv4vcC/wXWD/tdZ96zrq+RTw1Q3U/JvA9f22LwF+aWBeAb8wMH0m8K5+fH9gGXAisAK4HTi6n7cYeAj4OXA/8OVJ9jsPWA08baBtYb/OrsBvAN/pj8dtwCkDyy3oa9t67WMAbAW8D7gLuBk4dq1ljwZuBFb189/Wtz8J+BnwcF/z/cAewCnAp4Y8XrcAfwRcA/wE+Cyw7TqO+1eAE9bzuuwBnNf/zvwQOK5vP7A/Rg/1NX533L/3W/LgmYI21eOATwBPA/ake/P5q7WWORJ4C7A73ZvlhwGSzAW+CrwL2IXuTee8JHOG2O8rgL9f18wkzwTOAU4A5gAXAF9O8vgh/11PBZ4MzAWOAT6aZOeqWgJ8GvifVbV9Vb167RWrahlwMd2ZwYQjgAuq6i7gp3THZCe6gPgvSQ4doqbfBV4F7EP3Sfx1a81f0c/fkS4gPpjk+VX1U+Ag4Ed9zdtX1Y8GVxzyeB1G98b9dOC5wO+so87LgT9M8vtJnpMkA/t5HPBlug8Ac4EDgBOSvLKqvgb8OfDZvsbnDXFMNCKGgjZJVd1dVedV1QNVtQo4DXjZWot9sqqu69+c/gdwWN/v/2a6N8oLqurhqroQWAocPMSun0L3CX5d3kB3JnFhVT1E9wl7O+DFQ/7THgJOraqHquoCuk+uzxpyXei6iY6A9kb4230bVXVJVV3b/5uvoXszXvuYTeYw4ENVdVtV3UN3BtZU1Ver6gfVuRT4R+BXh6x3mOP14ar6Ub/vLwN7r2Nb7wb+ov83LwWWD1wsfwEwp6pOraqfV9XNwMeAw4esU9PEUNAmSfLEJP8rya1J7gMuA3Za62LvbQPjtwLb0HWjPA14fZJ7JwbgJXRnFBty9waW26PfFwBV9XBfx9xh/l3A3VW1emD6AWD7IdcF+Dywe5IX0XVHPZHurIgkL0xycZKVSX4C/B7d8diQPXj0sWySHJTk8iT39Mfy4CG3O7HtDR2vOwbG13k8qmpNVX20qvajOxs6DTgjyS/RveZ7rPWavxPYbcg6NU0MBW2qE+k+Qb+wqnYEXtq3Z2CZ+QPje9J9Cr+L7k3nk1W108DwpKp6zxD7/SfgteuZ/yO6N6CumK4LYz6wvG96gO6NesJTh9jnhA0+UriqHqDr3jqS7ozhM1X183723wHnA/Oru0D+N/z747Uut/PoYwlAkifQ9dO/D9itqnai6wKa2O6Gat7Q8dokVfWzqvoo3fWmhXSv+Q/Xes13qKqJs0Mf1zxDGAoaxjZJth0Ytqa7u+RnwL1JdgFOnmS9NydZmOSJwKnA31fVGrqLxa9O8sokW/Xb3D/JvCFqORl4cZL3JnkqQJJfSPKpJDsB5wK/keSAJNvQhdeDdBe1Aa4G3tTv90CG676ZcCfwH4dY7iy6bpnX8u/vOtoBuKeq/i3JvsCbhtzvucBxSeYl2Rk4aWDe44En0F28XZ3kIODX16r5KUmevJ5tr+94DS3JCf3ruF2Srfuuox3oLq5fAaxK8t/7+VsleXaSFwzUuaDvctMY+QJoGBfQBcDEcArwIbq+57voLjB+bZL1Pkl3d88dwLbAcQBVdRtwCF33wUq6T5F/zBC/j1X1A+BX6O7Wub7vhjmPrg97VVV9n+6axUf62l4NvHrg0/rxfdu9dH3fXxzyGAB8HFjYd3+sb73L6O7UWVZV3x5o/33g1CSrgD+he0MexseAr9NdpL2KrosKgP56znH9tn5MFzTnD8z/Ht21i5v7uvcY3PAQx2tjPAC8n+71vovuLqnXVtXN/YeBV9Fdj/hhP/9v6S7qA3yu/3l3Em9jHaNUedYmSep4piBJagwFSVJjKEiSGkNBktRs0qN3Z4pdd921FixYMO4yJGlWufLKK++qqkkfKzOrQ2HBggUsXbpRTySWpC1eklvXNc/uI0lSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVIzq7/RPIz/9Mdnj7uEGePK9x457hIkzXCeKUiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUjCwUksxPcnGSG5Jcn+T4vv2UJMuTXN0PBw+s844kNyX5fpJXjqo2SdLkRvk3mlcDJ1bVVUl2AK5McmE/74NV9b7BhZMsBA4HfhnYA/inJM+sqjUjrFGSNGBkZwpVdXtVXdWPrwJuBOauZ5VDgM9U1YNV9UPgJmDfUdUnSXq0abmmkGQBsA/wrb7p7UmuSXJGkp37trnAbQOrLWOSEEmyOMnSJEtXrlw5wqolacsz8lBIsj1wHnBCVd0HnA48A9gbuB14/8Zsr6qWVNWiqlo0Z86cKa9XkrZkIw2FJNvQBcKnq+rzAFV1Z1WtqaqHgY/xSBfRcmD+wOrz+jZJ0jQZ5d1HAT4O3FhVHxho331gsd8CruvHzwcOT/KEJE8H9gKuGFV9kqRHG+XdR/sBRwDXJrm6b3sn8MYkewMF3AK8DaCqrk9yLnAD3Z1Lx3rnkSRNr5GFQlV9A8gksy5YzzqnAaeNqiZJ0vr5jWZJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqRmZKGQZH6Si5PckOT6JMf37bskuTDJv/Q/d+7bk+TDSW5Kck2S54+qNknS5EZ5prAaOLGqFgIvAo5NshA4CbioqvYCLuqnAQ4C9uqHxcDpI6xNkjSJkYVCVd1eVVf146uAG4G5wCHAWf1iZwGH9uOHAGdX53JgpyS7j6o+SdKjTcs1hSQLgH2AbwG7VdXt/aw7gN368bnAbQOrLevb1t7W4iRLkyxduXLlyGqWpC3RyEMhyfbAecAJVXXf4LyqKqA2ZntVtaSqFlXVojlz5kxhpZKkkYZCkm3oAuHTVfX5vvnOiW6h/ueKvn05MH9g9Xl9myRpmozy7qMAHwdurKoPDMw6HziqHz8K+NJA+5H9XUgvAn4y0M0kSZoGW49w2/sBRwDXJrm6b3sn8B7g3CTHALcCh/XzLgAOBm4CHgCOHmFtkqRJjCwUquobQNYx+4BJli/g2FHVI0naML/RLElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqRnlH9nRZuZfT33OuEuYMfb8k2vHXYI0Ep4pSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJzVChkOSiYdokSbPber+nkGRb4InArkl2BtLP2hGYO+LaJEnTbENfXnsbcAKwB3Alj4TCfcBfjbAuSdIYrDcUquovgb9M8gdV9ZFpqkmSNCZDPeaiqj6S5MXAgsF1qursEdUlSRqDoUIhySeBZwBXA2v65gIMBUnajAz7QLxFwMKqqmE3nOQM4FXAiqp6dt92CvC7wMp+sXdW1QX9vHcAx9CFznFV9fVh9yVJmhrDfk/hOuCpG7ntM4EDJ2n/YFXt3Q8TgbAQOBz45X6dv06y1UbuT5L0GA17prArcEOSK4AHJxqr6jfXtUJVXZZkwZDbPwT4TFU9CPwwyU3AvsA3h1xfkjQFhg2FU6Zwn29PciSwFDixqn5M952HyweWWcY6vgeRZDGwGGDPPfecwrIkScPefXTpFO3vdODP6C5S/xnwfuAtG7OBqloCLAFYtGjR0Nc4JEkbNuzdR6vo3sgBHg9sA/y0qnbcmJ1V1Z0D2/wY8JV+cjkwf2DReX2bJGkaDXWhuap2qKod+xDYDngt8Ncbu7Mkuw9M/hbdBWyA84HDkzwhydOBvYArNnb7kqTHZqP/RnN/W+oXk5wMnLSu5ZKcA+xP99ykZcDJwP5J9qY767iF7jEaVNX1Sc4FbgBWA8dW1ZrJtitJGp1hu49eMzD5OLrvLfzb+tapqjdO0vzx9Sx/GnDaMPVIkkZj2DOFVw+Mr6b7lH/IlFcjbUH2+8h+4y5hxvjnP/jncZeg3rB3Hx096kIkSeM37B/ZmZfkC0lW9MN5SeaNujhJ0vQa9jEXn6C7Q2iPfvhy3yZJ2owMGwpzquoTVbW6H84E5oywLknSGAwbCncneXOSrfrhzcDdoyxMkjT9hg2FtwCHAXcAtwOvA35nRDVJksZk2FtSTwWO6h9eR5JdgPexkc8tkiTNbMOeKTx3IhAAquoeYJ/RlCRJGpdhQ+FxSXaemOjPFDb6ERmSpJlt2Df29wPfTPK5fvr1+EgKSdrsDPuN5rOTLAVe3je9pqpuGF1ZkqRxGLoLqA8Bg0CSNmPDXlOQJG0BDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqRlZKCQ5I8mKJNcNtO2S5MIk/9L/3LlvT5IPJ7kpyTVJnj+quiRJ6zbKM4UzgQPXajsJuKiq9gIu6qcBDgL26ofFwOkjrEuStA4jC4Wqugy4Z63mQ4Cz+vGzgEMH2s+uzuXATkl2H1VtkqTJTfc1hd2q6vZ+/A5gt358LnDbwHLL+rZHSbI4ydIkS1euXDm6SiVpCzS2C81VVUBtwnpLqmpRVS2aM2fOCCqTpC3XdIfCnRPdQv3PFX37cmD+wHLz+jZJ0jSa7lA4HziqHz8K+NJA+5H9XUgvAn4y0M0kSZomW49qw0nOAfYHdk2yDDgZeA9wbpJjgFuBw/rFLwAOBm4CHgCOHlVdkqR1G1koVNUb1zHrgEmWLeDYUdUiSRqO32iWJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkZutxFyBJU+HSl75s3CXMGC+77NJNXtczBUlSYyhIkhpDQZLUjOWaQpJbgFXAGmB1VS1KsgvwWWABcAtwWFX9eBz1SdKWapxnCr9WVXtX1aJ++iTgoqraC7ion5YkTaOZ1H10CHBWP34WcOgYa5GkLdK4QqGAf0xyZZLFfdtuVXV7P34HsNtkKyZZnGRpkqUrV66cjlolaYsxru8pvKSqlif5D8CFSb43OLOqKklNtmJVLQGWACxatGjSZSRJm2YsZwpVtbz/uQL4ArAvcGeS3QH6nyvGUZskbcmmPRSSPCnJDhPjwK8D1wHnA0f1ix0FfGm6a5OkLd04uo92A76QZGL/f1dVX0vybeDcJMcAtwKHjaE2SdqiTXsoVNXNwPMmab8bOGC665EkPWIm3ZIqSRozQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNTMuFJIcmOT7SW5KctK465GkLcmMCoUkWwEfBQ4CFgJvTLJwvFVJ0pZjRoUCsC9wU1XdXFU/Bz4DHDLmmiRpi5GqGncNTZLXAQdW1Vv76SOAF1bV2weWWQws7iefBXx/2gvdeLsCd427iM2Ix3PqeCyn1mw5nk+rqjmTzdh6uit5rKpqCbBk3HVsjCRLq2rRuOvYXHg8p47HcmptDsdzpnUfLQfmD0zP69skSdNgpoXCt4G9kjw9yeOBw4Hzx1yTJG0xZlT3UVWtTvJ24OvAVsAZVXX9mMuaCrOqu2sW8HhOHY/l1Jr1x3NGXWiWJI3XTOs+kiSNkaEgSWoMhRHzsR1TI8m2Sa5I8t0k1yf503HXNJslOSPJiiTXjbuWzUGS+UkuTnJD//t5/Lhr2lReUxih/rEd/w/4z8Ayurur3lhVN4y1sFkoSYAnVdX9SbYBvgEcX1WXj7m0WSnJS4H7gbOr6tnjrme2S7I7sHtVXZVkB+BK4NDZ+H/dM4XR8rEdU6Q69/eT2/SDn2g2UVVdBtwz7jo2F1V1e1Vd1Y+vAm4E5o63qk1jKIzWXOC2gellzNJflJkgyVZJrgZWABdW1bfGXZO0tiQLgH2AWfn7aSho1qiqNVW1N9033fdNYreHZpQk2wPnASdU1X3jrmdTGAqj5WM7RqCq7gUuBg4cdy3ShP5a13nAp6vq8+OuZ1MZCqPlYzumSJI5SXbqx7eju3j/vfFWJXX6GyE+DtxYVR8Ydz2PhaEwQlW1Gph4bMeNwLmbyWM7xmF34OIk19CF7YVV9ZUx1zRrJTkH+CbwrCTLkhwz7ppmuf2AI4CXJ7m6Hw4ed1GbwltSJUmNZwqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFqZdkzcDthFdPxVNtkyxI8qaB6UVJPvxYtyuNirekSr0k91fV9lO8zf2BP6qqV03ldqVR8UxB2oAktyR5d3/2sDTJ85N8PckPkvxev0ySvDfJdUmuTfKGfvX3AL/ar/tfk+yf5Cv9Orsk+WKSa5JcnuS5ffsp/d87uCTJzUmOG8+/XFuircddgDSDbNc/hXXCu6vqs/34v1bV3kk+CJxJ9w3WbYHrgL8BXgPsDTwP2BX4dpLLgJMYOFPozxwm/Cnwnao6NMnLgbP7bQD8IvBrwA7A95OcXlUPTfU/WFqboSA94mf9U1gnM/HMqmuB7ftn5q9K8mD/TKaXAOdU1RrgziSXAi8A1vekzJcArwWoqv+d5ClJduznfbWqHgQeTLIC2I3u0evSSNl9JA3nwf7nwwPjE9Oj+HA1uI81I9qH9CiGgjQ1/g/whv4PAc0BXgpcAayi6wJa1zq/Da1b6a7Z+gx+bT789CE9Yu1rCl+rqmFvS/0C8CvAd+n+TOh/q6o7ktwNrEnyXbprEd8ZWOcU4Iz+ya8PAEc9xvqlx8xbUiVJjd1HkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkpr/D5v+rKHCsQgbAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB5XAs5G7Ns6"
      },
      "source": [
        "# Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsRcj9lQ7Pks",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163,
          "referenced_widgets": [
            "b71eee35694d45e382f413fc7e0860c0",
            "d2bd87e624d54aaabd79638cba927554",
            "7c727193445949e78182cfad71c3bf91",
            "ac9ab7b2766e49f7861ff36936d773e3",
            "4dc4dd92c6a04809871d522956e09254",
            "2720f19d03db4a52a89afd130d8aa02a",
            "7a77b7a7a9374036aa45d0dabc438186",
            "eb3565aa1d894dfabee41acf5662f385",
            "7e5c6a536dda4a75ad84f5a86f83f5f7",
            "6c3fbf97bca24c2eb7c3484e38341414",
            "94c9c2068c724c9b92d9ff57eacdcac8",
            "d954668c045849528cf46d9612f5429d",
            "cf634502264a4a75ba0109d108dc2833",
            "d9de694ec4134e459f2f57e6c540ff22",
            "ecfa5e158e5f4af3acffc84c17f37acb",
            "7f6f7116a0d444f5aaca8f468427ec62",
            "4eea15228dfb4507b26fcf33879116e6",
            "b25a3835f4b846ee9d79d25febe002cd",
            "9224072aba08434ab60f26adae9dc9b5",
            "6f58aacd9bf44548b96525c4eaaece8c",
            "11a16c1e9b1e4ab3b7b02b59265f99f3",
            "c875daef99f34796a9ae777e3f20fa40",
            "8640c0d26a074311bf1a251bf41bfb2c",
            "173cfb3578804d33b204dd4243c2ce78"
          ]
        },
        "outputId": "d406ebaf-0834-49f6-e700-7eafd66ec69e"
      },
      "source": [
        "#Lets call the Bert pretrained tokenizer and the Bert Model\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "tokenizer = ppb.BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b71eee35694d45e382f413fc7e0860c0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e5c6a536dda4a75ad84f5a86f83f5f7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_wâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4eea15228dfb4507b26fcf33879116e6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1o8vnc3_qy2"
      },
      "source": [
        "# def BertTok(df):\n",
        "\n",
        "#   ### We need each list in tokenized to have the same length\n",
        "#   ### We do this by first getting the tweet with the biggest length\n",
        "#   ### and Padding the rest as per required to get each sentence to maxLen\n",
        "\n",
        "#   maxLen = 0\n",
        "#   for i in df.tweet.values:\n",
        "#     if len(i) > maxLen:\n",
        "#       maxLen = len(i)\n",
        "      \n",
        "#   print(maxLen)\n",
        "\n",
        "#   #tokenize\n",
        "#   tokenized = df['tweet'].apply((lambda x: tokenizer.encode_plus(x,\n",
        "#                                                                  max_length = maxLen,\n",
        "#                                                                  add_special_tokens=True,\n",
        "#                                                                  return_token_type_ids = False,\n",
        "#                                                                  pad_to_max_length=True,\n",
        "#                                                                  return_attention_mask=True,\n",
        "#                                                                  return_tensors='pt',\n",
        "#                                                                  )))\n",
        "\n",
        "#   print(tokenized[0])\n",
        "  \n",
        "\n",
        "#   #paded = np.array([i + [0]*(maxLen-len(i)) for i in tokenized.values])\n",
        "#   #print(paded.shape) #So just to be clear the maxLen was 35 as that is not the length of each numpy array.\n",
        "\n",
        "#   #We need to create another numpy array which gives 0 to ignore the mask and 1 to accept the Token\n",
        "#   #attention_mask = np.where(paded != 0,1,0)\n",
        "\n",
        "\n",
        "#   #print(attention_mask.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye2HgV_OAv0v"
      },
      "source": [
        "# emot_trainFeat = BertTok(emot_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCrZi62D5055"
      },
      "source": [
        "### From torch import Dataset and create pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjNBSM3F58uC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "5990d5e4-9d67-4d9f-dde4-200149ae8cd9"
      },
      "source": [
        "#We are going to turn the pandas dataframes into Pytorch Datasets using the\n",
        "#TweetDataset class.\n",
        "class TweetDataset(Dataset):\n",
        "  ##Create constructor\n",
        "  def __init__(self, tweets, labels, tokenizer, max_len):\n",
        "    self.tweets = tweets\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  ##Mandatory function for length of dataset\n",
        "  def  __len__(self):\n",
        "    return len(self.tweets)\n",
        "\n",
        "  ##Mandatory getter function to retrieve item\n",
        "  def __getitem__(self,item):\n",
        "    tweet = str(self.tweets[item])\n",
        "    label = int(self.labels[item])\n",
        "\n",
        "    ##Encoding function from transformers tokenizers with extra parameter settings to pad, return attention mask etc etc\n",
        "    encoding = self.tokenizer.encode_plus(tweet,\n",
        "                                    max_length = self.max_len,\n",
        "                                    add_special_tokens=True,\n",
        "                                    return_token_type_ids = False,\n",
        "                                    pad_to_max_length=True,\n",
        "                                    return_attention_mask=True,\n",
        "                                    return_tensors='pt',\n",
        "                          )\n",
        "  \n",
        "    ##Return the transformed input Ids and the attention mask along with tensored labels\n",
        "    return {\n",
        "        'tweet_text':tweet,\n",
        "        'input_ids':encoding['input_ids'].flatten(),\n",
        "        'attention_mask':encoding['attention_mask'].flatten(),\n",
        "        'labels':torch.tensor(label, dtype=torch.long)\n",
        "    }\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1c470b51719a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#We are going to turn the pandas dataframes into Pytorch Datasets using the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTweetDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;31m##Create constructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej_8Oc6nIT6t",
        "outputId": "7fe761fd-df2f-424e-f4d1-635813637208"
      },
      "source": [
        "emot_train.shape,emot_val.shape, emot_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2605, 2), (652, 2), (1421, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEs9E-RlZH7Z"
      },
      "source": [
        "### Now we create the Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UXxfWzcZL2S"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = TweetDataset(\n",
        "      tweets = df.tweet.to_numpy(),\n",
        "      labels = df.label.to_numpy(),\n",
        "      tokenizer = tokenizer,\n",
        "      max_len = max_len\n",
        "  )\n",
        "  \n",
        "  return DataLoader(\n",
        "      ds,\n",
        "      batch_size = batch_size,\n",
        "      num_workers = 2\n",
        "  )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erXIqm1JAZ86"
      },
      "source": [
        "MAX_LEN = 163\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGzSSk62bsLq"
      },
      "source": [
        "#Lets call the Bert pretrained tokenizer and the Bert Model\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "tokenizer = ppb.BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "## Lets call the Data Loader for all of our datasets train,val,test\n",
        "emot_train_dataload = create_data_loader(emot_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "emot_val_dataload = create_data_loader(emot_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "emot_test_dataload = create_data_loader(emot_test, tokenizer, MAX_LEN, BATCH_SIZE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEoCCbfrD_tb"
      },
      "source": [
        "###Just checking the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9fcLU80oKL0",
        "outputId": "83dd32be-bc0d-414f-c3ac-077403624ded"
      },
      "source": [
        "data = next(iter(emot_train_dataload))\n",
        "print(data.keys())\n",
        "print(data['input_ids'][0])\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['labels'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['tweet_text', 'input_ids', 'attention_mask', 'labels'])\n",
            "tensor([  101,   146,   112,   182,  2296,  1103,  6129,  1269,  1236,   119,\n",
            "         2907,   146,  2373,  1142,  1107,   138, 19890,   112,   188,  1490,\n",
            "          117,  1122,  2136,  1143,  1114,   170,  1762,  1183, 16305,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0])\n",
            "torch.Size([16, 163])\n",
            "torch.Size([16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVZhMt3d0bVl"
      },
      "source": [
        "###Now lets call the BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114,
          "referenced_widgets": [
            "11ed57caa879459b985d47aff71446d9",
            "c23f9bc5770e48d8944ef4b3cf4a3afb",
            "f4b1c639c8e84dd7ae986737a94ab08e",
            "3ab2c3ca6cd343f79a6a5d337e530e87",
            "9ef785cf76954103b95420716b4a1e95",
            "e655717e7fbd4949a6eeae96cf912ff2",
            "748cc0a00d1d46fa987fc2a0d76e15a6",
            "d84ae0d177244b3b87b44d3f5fc66cfa",
            "1b439de4c7a1486094db4fc9e4fc1cba",
            "b139b8e979d94d6497bcf87aae88e15f",
            "41b86e4b6c84458abe93ca90fa783052",
            "ff06024d402a46d3a1ed537450318f4f",
            "588b25c8d7a24236a62ead8f1d0aa50d",
            "f04d333ffcae4c48bfb57e613072e0de",
            "d2eeb29a4118449a86c0107435f51cad",
            "98ab349d3e924eff99c94e043fa39719"
          ]
        },
        "id": "73sWDt5L0ag_",
        "outputId": "3c1c1645-773e-42cf-87ac-059308e602c1"
      },
      "source": [
        "bert_model = ppb.BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME,return_dict=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11ed57caa879459b985d47aff71446d9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b439de4c7a1486094db4fc9e4fc1cba",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olk3pXgcMPZr"
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = ppb.BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict = False)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out= nn.Linear(self.bert.config.hidden_size,n_classes)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(  #last_hidden_state is denoted as _ as we will not use it\n",
        "    input_ids = input_ids,\n",
        "    attention_mask = attention_mask\n",
        "    )\n",
        "    output=self.drop(pooled_output)\n",
        "    output=self.out(output)\n",
        "    return self.softmax(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj0IBmwSe94w"
      },
      "source": [
        "model = SentimentClassifier(len(emot_data[2]))\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCZ34Sh_DvLQ"
      },
      "source": [
        "###Lets start with the training of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7Jly9Dwg7RU",
        "outputId": "c475971c-78d2-4476-f0cd-30f44478a5d0"
      },
      "source": [
        "## Just a test\n",
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "print(input_ids.shape,attention_mask.shape)\n",
        "print(type(input_ids[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 163]) torch.Size([16, 163])\n",
            "<class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilkt6AExLWan",
        "outputId": "9b3dede9-ee27-4f77-bbd5-e9e16f98d8c2"
      },
      "source": [
        "model(input_ids,attention_mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1216, 0.5032, 0.1941, 0.1811],\n",
              "        [0.1204, 0.3219, 0.3817, 0.1760],\n",
              "        [0.0820, 0.3960, 0.2768, 0.2452],\n",
              "        [0.1269, 0.4234, 0.1958, 0.2539],\n",
              "        [0.2188, 0.4065, 0.1676, 0.2071],\n",
              "        [0.1723, 0.3579, 0.2295, 0.2403],\n",
              "        [0.1454, 0.3026, 0.3348, 0.2171],\n",
              "        [0.1459, 0.2909, 0.2906, 0.2726],\n",
              "        [0.1021, 0.3117, 0.3291, 0.2571],\n",
              "        [0.1820, 0.5327, 0.1379, 0.1474],\n",
              "        [0.1802, 0.3189, 0.2270, 0.2738],\n",
              "        [0.2701, 0.2271, 0.2696, 0.2332],\n",
              "        [0.1128, 0.4786, 0.1633, 0.2453],\n",
              "        [0.1036, 0.3261, 0.2644, 0.3060],\n",
              "        [0.1298, 0.3568, 0.2655, 0.2479],\n",
              "        [0.1289, 0.3704, 0.2317, 0.2690]], device='cuda:0',\n",
              "       grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBQbVwQPQK5Y"
      },
      "source": [
        "#Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY--C7AbQH76"
      },
      "source": [
        "#In line with the paper that introduced BERT we are going to use the Adam algorithm with weight decay fix as our optimizer\n",
        "\n",
        "optimizer = ppb.AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(emot_train_dataload)*EPOCHS\n",
        "\n",
        "\n",
        "scheduler = ppb.get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VIrPQDdUQO8"
      },
      "source": [
        "Helper function for training our model for one(each) EPOCH:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YRinkTnUc4H"
      },
      "source": [
        "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
        "  model = model.train()\n",
        "  \n",
        "  losses = []\n",
        "  correct_pred = 0\n",
        "\n",
        "  for d in data_loader:\n",
        "    input_ids = d['input_ids'].to(device)\n",
        "    attention_mask = d['attention_mask'].to(device)\n",
        "    labels = d['labels'].to(device)\n",
        "\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    _,preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "\n",
        "    correct_pred += torch.sum(preds==labels)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_pred.double()/n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCQ8WFtCYSUt"
      },
      "source": [
        "Training the model should look familiar, except for two things. The scheduler gets called every time a batch is fed to the model. We're avoiding exploding gradients by clipping the gradients of the model using clip_grad_norm_.\n",
        "\n",
        "Let's write another one that helps us evaluate the model on a given data loader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dYlBSyEYTXd"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_pred = 0\n",
        "\n",
        "  for d in data_loader:\n",
        "    input_ids = d['input_ids'].to(device)\n",
        "    attention_mask = d['attention_mask'].to(device)\n",
        "    labels = d['labels'].to(device)\n",
        "\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    _,preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "\n",
        "    correct_pred += torch.sum(preds == labels)\n",
        "    losses.append(loss.item())\n",
        "  return correct_pred.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJjrFrIddGNC"
      },
      "source": [
        "Using the above 2 helper functions and the declaration of the optimizer, scheduler and loss function we can now write our training loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acHk8kYGdF0y",
        "outputId": "2660797c-a27a-4301-e09a-c791043e2436"
      },
      "source": [
        "%%time\n",
        "torch.cuda.empty_cache()\n",
        "history = collections.defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(model,emot_train_dataload,loss_fn,optimizer,\n",
        "                                      device,scheduler,len(emot_train))\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(model,emot_val_dataload,loss_fn, device,\n",
        "                                 len(emot_val))\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  #We are determining the best model using the validation accuracy and then\n",
        "  #storing the state of the best model.\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 1.1121581138277346 accuracy 0.6245681381957774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.0070665958451062 accuracy 0.7331288343558282\n",
            "\n",
            "Epoch 2/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.9905576168393796 accuracy 0.7508637236084453\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.0015937220759508 accuracy 0.7392638036809817\n",
            "\n",
            "Epoch 3/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.9439785568260708 accuracy 0.7980806142034549\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9774145222291714 accuracy 0.7668711656441718\n",
            "\n",
            "Epoch 4/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8995017862758754 accuracy 0.8426103646833014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9707771583301265 accuracy 0.7699386503067485\n",
            "\n",
            "Epoch 5/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.866985384671966 accuracy 0.8752399232245682\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9651103775675703 accuracy 0.7776073619631902\n",
            "\n",
            "Epoch 6/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8481562550813874 accuracy 0.8944337811900193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.97638340403394 accuracy 0.7638036809815951\n",
            "\n",
            "Epoch 7/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8381224947473023 accuracy 0.9047984644913628\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9598903714156732 accuracy 0.7822085889570553\n",
            "\n",
            "Epoch 8/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8287523169459009 accuracy 0.9143953934740884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9787873480378128 accuracy 0.7607361963190185\n",
            "\n",
            "Epoch 9/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.822065105833159 accuracy 0.9209213051823417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9646718763723606 accuracy 0.7776073619631902\n",
            "\n",
            "Epoch 10/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8124677736335005 accuracy 0.9312859884836853\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9667764175228957 accuracy 0.7760736196319019\n",
            "\n",
            "Epoch 11/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8097644810296275 accuracy 0.9339731285988484\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9666093006366636 accuracy 0.7730061349693251\n",
            "\n",
            "Epoch 12/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8097537770593093 accuracy 0.9343570057581574\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9515887411629281 accuracy 0.7898773006134969\n",
            "\n",
            "Epoch 13/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.807174564504916 accuracy 0.9362763915547025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9543407879224638 accuracy 0.7883435582822086\n",
            "\n",
            "Epoch 14/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8042437840093133 accuracy 0.9393474088291747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9584782036339364 accuracy 0.7837423312883436\n",
            "\n",
            "Epoch 15/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.805153294574995 accuracy 0.9385796545105567\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9584161333921479 accuracy 0.7837423312883436\n",
            "\n",
            "Epoch 16/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8039694856280929 accuracy 0.9393474088291747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9502084284293942 accuracy 0.7929447852760736\n",
            "\n",
            "Epoch 17/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8030938590231117 accuracy 0.9408829174664108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9552570072616019 accuracy 0.7868098159509203\n",
            "\n",
            "Epoch 18/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8030241647380992 accuracy 0.9404990403071017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9558736013203133 accuracy 0.7852760736196319\n",
            "\n",
            "Epoch 19/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.8024312140751471 accuracy 0.9412667946257198\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9552647747644564 accuracy 0.7868098159509203\n",
            "\n",
            "Epoch 20/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.801855198810437 accuracy 0.9420345489443379\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9576326623195555 accuracy 0.7822085889570553\n",
            "\n",
            "CPU times: user 29min 55s, sys: 21min 28s, total: 51min 23s\n",
            "Wall time: 51min 27s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "oZSPAdNfy3Hf",
        "outputId": "916b5e78-f334-4520-cfbd-a8623a6e123f"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LIYq99QzJjb"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D45KULcvzPSX",
        "outputId": "bfd03ab4-2d18-4c34-c28d-72edda11d218"
      },
      "source": [
        "test_acc, _ = eval_model(model,emot_test_dataload,loss_fn,device,len(emot_test))\n",
        "test_acc.item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7797325826882477"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_lgdXWY2Pjr"
      },
      "source": [
        "def get_predictions(model,data_loader):\n",
        "  model = model.eval()\n",
        "\n",
        "  tweet_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      tweets = d[\"tweet_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      labels = d[\"labels\"].to(device)\n",
        "\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "      _,preds = torch.max(outputs, dim=1)\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      tweet_texts.extend(tweets)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(labels)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "\n",
        "  return tweet_texts, predictions, prediction_probs, real_values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFpjHcqYATKn",
        "outputId": "264b1a23-a1e3-4281-85e5-99d637cc28e7"
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  emot_test_dataload\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mjtCuISBXjP",
        "outputId": "6a5cb0ad-5fd9-428c-b944-606273545d74"
      },
      "source": [
        "print(classification_report(y_test, y_pred, target_names=label_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.78      0.88      0.83       558\n",
            "         joy       0.85      0.73      0.79       358\n",
            "    optimism       0.54      0.56      0.55       123\n",
            "     sadness       0.81      0.75      0.77       382\n",
            "\n",
            "    accuracy                           0.78      1421\n",
            "   macro avg       0.74      0.73      0.74      1421\n",
            "weighted avg       0.78      0.78      0.78      1421\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "UsQSglGiB8wB",
        "outputId": "988398f8-7aa3-4892-cfc0-3f6d99feb291"
      },
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True Emotion')\n",
        "  plt.xlabel('Predicted Emotion');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}